{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#3.1-中文分词简介\" data-toc-modified-id=\"3.1-中文分词简介-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>3.1 中文分词简介</a></span></li><li><span><a href=\"#3.2-规则分词\" data-toc-modified-id=\"3.2-规则分词-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>3.2 规则分词</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.2.1-正向最大匹配法（Maximum-Match-Method，MM法）\" data-toc-modified-id=\"3.2.1-正向最大匹配法（Maximum-Match-Method，MM法）-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>3.2.1 正向最大匹配法（Maximum Match Method，MM法）</a></span></li><li><span><a href=\"#3.2.2-逆向最大匹配法（Reverse-Maximum-Match-Method，-RMM法）\" data-toc-modified-id=\"3.2.2-逆向最大匹配法（Reverse-Maximum-Match-Method，-RMM法）-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>3.2.2 逆向最大匹配法（Reverse Maximum Match Method， RMM法）</a></span></li><li><span><a href=\"#3.2.3-双向最大匹配法（Bi-directction-Matching-method）\" data-toc-modified-id=\"3.2.3-双向最大匹配法（Bi-directction-Matching-method）-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>3.2.3 双向最大匹配法（Bi-directction Matching method）</a></span></li></ul></li><li><span><a href=\"#3.3-统计分词\" data-toc-modified-id=\"3.3-统计分词-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3.3 统计分词</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1-语言模型\" data-toc-modified-id=\"3.3.1-语言模型-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>3.3.1 语言模型</a></span></li><li><span><a href=\"#3.3.2-HMM模型\" data-toc-modified-id=\"3.3.2-HMM模型-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>3.3.2 HMM模型</a></span></li><li><span><a href=\"#3.3.3-其他统计分词算法\" data-toc-modified-id=\"3.3.3-其他统计分词算法-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>3.3.3 其他统计分词算法</a></span></li></ul></li><li><span><a href=\"#3.4-混合分词\" data-toc-modified-id=\"3.4-混合分词-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>3.4 混合分词</a></span></li><li><span><a href=\"#3.5-中文分词工具——jieba\" data-toc-modified-id=\"3.5-中文分词工具——jieba-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>3.5 中文分词工具——jieba</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.5.1-jieba的三种分词模式\" data-toc-modified-id=\"3.5.1-jieba的三种分词模式-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>3.5.1 jieba的三种分词模式</a></span></li><li><span><a href=\"#3.5.2-添加自定义词典\" data-toc-modified-id=\"3.5.2-添加自定义词典-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>3.5.2 添加自定义词典</a></span></li><li><span><a href=\"#载入词典\" data-toc-modified-id=\"载入词典-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>载入词典</a></span></li><li><span><a href=\"#调整词典\" data-toc-modified-id=\"调整词典-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>调整词典</a></span></li><li><span><a href=\"#3.5.3-关键词提取\" data-toc-modified-id=\"3.5.3-关键词提取-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>3.5.3 关键词提取</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于-TF-IDF-算法的关键词提取\" data-toc-modified-id=\"基于-TF-IDF-算法的关键词提取-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>基于 TF-IDF 算法的关键词提取</a></span></li><li><span><a href=\"#基于-TextRank-算法的关键词提取\" data-toc-modified-id=\"基于-TextRank-算法的关键词提取-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>基于 TextRank 算法的关键词提取</a></span></li></ul></li><li><span><a href=\"#3.5.4-自定义语料库\" data-toc-modified-id=\"3.5.4-自定义语料库-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>3.5.4 自定义语料库</a></span></li><li><span><a href=\"#3.5.5-词性标注\" data-toc-modified-id=\"3.5.5-词性标注-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>3.5.5 词性标注</a></span><ul class=\"toc-item\"><li><span><a href=\"#词性对照表（allowPOS可选值）\" data-toc-modified-id=\"词性对照表（allowPOS可选值）-5.7.1\"><span class=\"toc-item-num\">5.7.1&nbsp;&nbsp;</span>词性对照表（allowPOS可选值）</a></span><ul class=\"toc-item\"><li><span><a href=\"#名词-(1个一类，7个二类，5个三类)\" data-toc-modified-id=\"名词-(1个一类，7个二类，5个三类)-5.7.1.1\"><span class=\"toc-item-num\">5.7.1.1&nbsp;&nbsp;</span>名词 (1个一类，7个二类，5个三类)</a></span></li><li><span><a href=\"#时间词(1个一类，1个二类)\" data-toc-modified-id=\"时间词(1个一类，1个二类)-5.7.1.2\"><span class=\"toc-item-num\">5.7.1.2&nbsp;&nbsp;</span>时间词(1个一类，1个二类)</a></span></li><li><span><a href=\"#处所词(1个一类)\" data-toc-modified-id=\"处所词(1个一类)-5.7.1.3\"><span class=\"toc-item-num\">5.7.1.3&nbsp;&nbsp;</span>处所词(1个一类)</a></span></li><li><span><a href=\"#方位词(1个一类)\" data-toc-modified-id=\"方位词(1个一类)-5.7.1.4\"><span class=\"toc-item-num\">5.7.1.4&nbsp;&nbsp;</span>方位词(1个一类)</a></span></li><li><span><a href=\"#动词(1个一类，9个二类)\" data-toc-modified-id=\"动词(1个一类，9个二类)-5.7.1.5\"><span class=\"toc-item-num\">5.7.1.5&nbsp;&nbsp;</span>动词(1个一类，9个二类)</a></span></li><li><span><a href=\"#形容词(1个一类，4个二类)\" data-toc-modified-id=\"形容词(1个一类，4个二类)-5.7.1.6\"><span class=\"toc-item-num\">5.7.1.6&nbsp;&nbsp;</span>形容词(1个一类，4个二类)</a></span></li><li><span><a href=\"#区别词(1个一类，2个二类)\" data-toc-modified-id=\"区别词(1个一类，2个二类)-5.7.1.7\"><span class=\"toc-item-num\">5.7.1.7&nbsp;&nbsp;</span>区别词(1个一类，2个二类)</a></span></li><li><span><a href=\"#状态词(1个一类)\" data-toc-modified-id=\"状态词(1个一类)-5.7.1.8\"><span class=\"toc-item-num\">5.7.1.8&nbsp;&nbsp;</span>状态词(1个一类)</a></span></li><li><span><a href=\"#代词(1个一类，4个二类，6个三类)\" data-toc-modified-id=\"代词(1个一类，4个二类，6个三类)-5.7.1.9\"><span class=\"toc-item-num\">5.7.1.9&nbsp;&nbsp;</span>代词(1个一类，4个二类，6个三类)</a></span></li><li><span><a href=\"#数词(1个一类，1个二类)\" data-toc-modified-id=\"数词(1个一类，1个二类)-5.7.1.10\"><span class=\"toc-item-num\">5.7.1.10&nbsp;&nbsp;</span>数词(1个一类，1个二类)</a></span></li><li><span><a href=\"#量词(1个一类，2个二类)\" data-toc-modified-id=\"量词(1个一类，2个二类)-5.7.1.11\"><span class=\"toc-item-num\">5.7.1.11&nbsp;&nbsp;</span>量词(1个一类，2个二类)</a></span></li><li><span><a href=\"#副词(1个一类)\" data-toc-modified-id=\"副词(1个一类)-5.7.1.12\"><span class=\"toc-item-num\">5.7.1.12&nbsp;&nbsp;</span>副词(1个一类)</a></span></li><li><span><a href=\"#介词(1个一类，2个二类)\" data-toc-modified-id=\"介词(1个一类，2个二类)-5.7.1.13\"><span class=\"toc-item-num\">5.7.1.13&nbsp;&nbsp;</span>介词(1个一类，2个二类)</a></span></li><li><span><a href=\"#连词(1个一类，1个二类)\" data-toc-modified-id=\"连词(1个一类，1个二类)-5.7.1.14\"><span class=\"toc-item-num\">5.7.1.14&nbsp;&nbsp;</span>连词(1个一类，1个二类)</a></span></li><li><span><a href=\"#助词(1个一类，15个二类)\" data-toc-modified-id=\"助词(1个一类，15个二类)-5.7.1.15\"><span class=\"toc-item-num\">5.7.1.15&nbsp;&nbsp;</span>助词(1个一类，15个二类)</a></span></li><li><span><a href=\"#叹词(1个一类)\" data-toc-modified-id=\"叹词(1个一类)-5.7.1.16\"><span class=\"toc-item-num\">5.7.1.16&nbsp;&nbsp;</span>叹词(1个一类)</a></span></li><li><span><a href=\"#语气词(1个一类)\" data-toc-modified-id=\"语气词(1个一类)-5.7.1.17\"><span class=\"toc-item-num\">5.7.1.17&nbsp;&nbsp;</span>语气词(1个一类)</a></span></li><li><span><a href=\"#拟声词(1个一类)\" data-toc-modified-id=\"拟声词(1个一类)-5.7.1.18\"><span class=\"toc-item-num\">5.7.1.18&nbsp;&nbsp;</span>拟声词(1个一类)</a></span></li><li><span><a href=\"#后缀(1个一类)\" data-toc-modified-id=\"后缀(1个一类)-5.7.1.19\"><span class=\"toc-item-num\">5.7.1.19&nbsp;&nbsp;</span>后缀(1个一类)</a></span></li><li><span><a href=\"#字符串(1个一类，2个二类)\" data-toc-modified-id=\"字符串(1个一类，2个二类)-5.7.1.20\"><span class=\"toc-item-num\">5.7.1.20&nbsp;&nbsp;</span>字符串(1个一类，2个二类)</a></span></li><li><span><a href=\"#标点符号(1个一类，16个二类)\" data-toc-modified-id=\"标点符号(1个一类，16个二类)-5.7.1.21\"><span class=\"toc-item-num\">5.7.1.21&nbsp;&nbsp;</span>标点符号(1个一类，16个二类)</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章 中文分词技术\n",
    "\n",
    "## 3.1 中文分词简介\n",
    "中文自动分词主要归纳为：\n",
    "\n",
    "+ 规则分词。最早兴起的方法，主要是通过人工设立词库，按照一定方式进行匹配切分你，其实现简单高效，但是对新词很难进行处理\n",
    "\n",
    "+ 统计分词。能够较好的应对新词发现等特殊场景，但是，太过于依赖语料的质量\n",
    "\n",
    "+ 混合分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 规则分词\n",
    "\n",
    "基于规则的分词是一种机械分词方法，主要是通过维护词典，在切分语句时，将语句的每个字符串与词表中的词进行逐一匹配，找到则切分，否则不予以切分。\n",
    "\n",
    "按照匹配切分的方式，主要有：\n",
    "\n",
    "+ 正向最大匹配法\n",
    "\n",
    "+ 逆向最大匹配法\n",
    "\n",
    "+ 双向最大匹配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 正向最大匹配法（Maximum Match Method，MM法）\n",
    "\n",
    "假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理。如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。\n",
    "\n",
    "### 3.2.2 逆向最大匹配法（Reverse Maximum Match Method， RMM法）\n",
    "\n",
    "基本原理和MM法相同，不同的是分词切分的方向与MM法相反，从被处理文档的末端开始匹配扫描。\n",
    "\n",
    "由于汉语中偏正结果较多，若从后向前匹配，可以适当提高精确度。统计结果表明，单纯使用正向最大匹配的错误率为1/169，单纯使用逆向最大匹配的错误率为1/245。\n",
    "\n",
    "### 3.2.3 双向最大匹配法（Bi-directction Matching method）\n",
    "\n",
    "将正向最大匹配法得到的分词结果和逆向最大匹配法得到的结果进行比较，然后按照最大匹配原则，选取词数切分最少的作为结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['南京市', '长江大桥']\n"
     ]
    }
   ],
   "source": [
    "# 逆向最大匹配\n",
    "class IMM(object):\n",
    "    def __init__(self, dic_path):\n",
    "        self.dictionary = set()\n",
    "        self.maximum = 0\n",
    "        # 读取词典\n",
    "        with open(dic_path, 'r', encoding= 'utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                self.dictionary.add(line)\n",
    "                self.maximum = max(len(line),self.maximum)\n",
    "    def cut(self, text):\n",
    "        result = []\n",
    "        index = len(text)\n",
    "        while index > 0:\n",
    "            word = None\n",
    "            for size in range(self.maximum, 0, -1):  #9，8，7\n",
    "                if index - size < 0:\n",
    "                    continue\n",
    "                piece = text[(index - size):index]\n",
    "                if piece in self.dictionary:\n",
    "                    word = piece\n",
    "                    result.append(word)\n",
    "                    index -= size\n",
    "                    break\n",
    "            if word is None:\n",
    "                index -= 1\n",
    "        return result[::-1]\n",
    "def main():\n",
    "    text = \"南京市长江大桥\"\n",
    "    tokenizer = IMM('./data/imm_dic.utf8')\n",
    "    print(tokenizer.cut(text))\n",
    "\n",
    "main()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['南京市长', '江', '大桥']\n"
     ]
    }
   ],
   "source": [
    "# 正向最大匹配\n",
    "class MM(object):\n",
    "    def __init__(self, dic_path):\n",
    "        self.dictionary = set()\n",
    "        self.maximun = 0\n",
    "        with open(dic_path, 'r', encoding= 'utf8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                self.dictionary.add(line)\n",
    "                self.maximun = max(len(line), self.maximun)\n",
    "    def cut(self, text):\n",
    "        result = []\n",
    "        index = len(text)\n",
    "        str = 0 \n",
    "        size = 0\n",
    "        while str + size -1  < 80:\n",
    "            word = None\n",
    "            for size in range(self.maximun , 0, -1):\n",
    "                if index - size < 0:\n",
    "                    continue\n",
    "                piece = text[str:str+size]\n",
    "                if piece in self.dictionary:\n",
    "                    word = piece\n",
    "                    result.append(word)\n",
    "                    str += size\n",
    "                    break\n",
    "            if word is None:\n",
    "                str += 1\n",
    "        return result[::]\n",
    "def mi():\n",
    "    text = \"南京市长江大桥\"\n",
    "    tokenizer = MM('./data/imm_dic copy.utf8')\n",
    "    print(tokenizer.cut(text))\n",
    "\n",
    "mi()        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 统计分词\n",
    "\n",
    "统计语料中相邻共现的各个字的组合的频度，当组合频度高于某一个临界值时，我们便可认为字组可能会构成一个词语。\n",
    "\n",
    "基于统计的分词，一般要做如下两步操作：\n",
    "\n",
    "1）建立统计语言模型\n",
    "\n",
    "2）对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里就用到了统计学校算法，如隐含马尔可夫（HMM)、条件随机场（CRF）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 语言模型\n",
    "\n",
    "用概率论的专业术语描述语言模型就是：为长度为m的字符串确定其概率分布$P(\\omega_1, \\omega_2, \\dots, \\omega_m)$，其中$\\omega_1$到$\\omega_m$依次表示文本中的各个词语。一般采用链式法则计算其概率值，如下式所示：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{split}\n",
    "{} & P(\\omega_1, \\omega_2, \\dots, \\omega_m) = P(\\omega_1)P(\\omega_2|\\omega_1)P(\\omega_3|\\omega_1,\\omega_2){}\\\\\n",
    "\t&\\dots P(\\omega_i|\\omega_1,\\omega_2, \\dots, \\omega_(i-1))\\dots P(\\omega_m|\\omega_1, \\omega_2, \\dots, \\omega_(m-1)) \n",
    "\\end{split} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式从第三项起的每一项计算难度都很大。为解决该问题，有人提出n元模型（n-gram mmodel）降低该计算难度。_\n",
    "所谓n元模型就是在估算条件概率时，忽略距离大于等于n的上文词的影响，所以可以简化为：\n",
    "\n",
    "$$ P(\\omega_i|\\omega_1, \\omega_2, \\dots, \\omega_(i-1)) \\approx P(\\omega_i|\\omega_1, \\omega_(i-(n-1)), \\dots, \\omega_(i-1)) \\tag{3.2}$$\n",
    "\n",
    "当 n = 1时，称为一元模型（unigram medel），此时整个句子的概率可表示为：$P(\\omega_1, \\omega_2, \\dots, \\omega_m) = P(\\omega_1)P(\\omega_2)\\dots P(\\omega_m)$，即在一元语言模型中，整个句子的概率等于各个词语概率的乘积，即各个词之间都是相互独立的，损失了句中的语序信息。\n",
    "\n",
    "当 n = 2时，称为二元模型（bigram model），式（3.2）变为$P(\\omega_i|\\omega_1, \\omega_2, \\dots, \\omega_(i-1)) = P(\\omega_i|\\omega_(i-1))$\n",
    "\n",
    "当 n = 3时，称为三元模型（trigram model），式（3.2）变为$P(\\omega_i|\\omega_1, \\omega_2, \\dots, \\omega_(i-1)) = P(\\omega_i|\\omega_(i-2), \\omega_(i-1))$\n",
    "\n",
    "显然当n $\\geq$ 2时，该模型是可以保留一定的词序信息的，而且n越大，保留的词序信息约丰富，但计算成本也呈指数级增长。\n",
    "\n",
    "一般使用频率计数的比例来计算n元条件概率，如式（3.3）所示：\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(\\omega_i|\\omega_(i-(n-1)), \\dots, \\omega_(i-1)) = \\frac{count(\\omega_(i-(n-1)), \\omega_(i-1), \\omega_i)}{count(\\omega_(i-(n-1)),\\dots, \\omega_(i-1))} \\tag{3.3}$$\n",
    "\n",
    "式子中$count(\\omega_(i-(n-1)), \\dots, \\omega_(i-1))$表示词语$\\omega_(i-(n-1)), \\dots, \\omega_(i-1)$在语料库中出现的总次数。\n",
    "\n",
    "n越大，模型包含的词序信息约丰富，计算量也越大，同时，长度越长的文本序列出现的次数也会减少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 HMM模型\n",
    "隐含马尔可夫模型（HMM）是将分词作为字，在字串中的序列标注任务来实现的。\n",
    "\n",
    "思路：每个字在构造一个特定的词语时都占据着一个确定的构词位置（即词位），现规定每个字最多只有四个构词位置：即B（词首）、M（词中）、E（词尾）、S（单独成词）。\n",
    "例子：\n",
    "\n",
    "1）中文/分词/是/文本处理/不可或缺/的/一步！\n",
    "\n",
    "2）中/B 文/E 分/B 词/E 是/S 文/B 本/M 处/M 理/E 不/B 可/M 或/M 缺/E 的/S 一/B 步/E ！/S\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【未启用 HMM】：他/ 来到/ 了/ 网易/ 杭/ 研/ 大厦\n",
      "【识别新词】：他/ 来到/ 了/ 网易/ 杭研/ 大厦\n"
     ]
    }
   ],
   "source": [
    "# 未启用 HMM\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\", HMM=False) #默认精确模式和启用 HMM\n",
    "print(\"【未启用 HMM】：\" + \"/ \".join(seg_list))\n",
    "\n",
    "# 识别新词\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") #默认精确模式和启用 HMM\n",
    "print(\"【识别新词启用 HMM】：\" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def try_load_model(self, trained):\n",
    "        pass\n",
    "    \n",
    "    def train(self, path):\n",
    "        pass\n",
    "    \n",
    "    def viterbi(self, text, states, start_p, emit_p):\n",
    "        pass\n",
    "    \n",
    "    def cut(self, text):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_init_主要是初始化一些全局信息，用于初始化一些成员变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self):\n",
    "        import os\n",
    "\n",
    "        # 主要是用于存取算法中间结果，不用每次都训练模型\n",
    "        self.model_file = 'learning-nlp/chapter-3/data/hmm_model.pkl'\n",
    "\n",
    "        # 状态值集合\n",
    "        self.state_list = ['B', 'M', 'E', 'S']\n",
    "        # 参数加载,用于判断是否需要重新加载model_file\n",
    "        self.load_para = False\n",
    "\n",
    "    # 用于加载已计算的中间结果，当需要重新训练时，需初始化清空结果\n",
    "    def try_load_model(self, trained):\n",
    "        if trained:\n",
    "            import pickle\n",
    "            with open(self.model_file, 'rb') as f:\n",
    "                self.A_dic = pickle.load(f)\n",
    "                self.B_dic = pickle.load(f)\n",
    "                self.Pi_dic = pickle.load(f)\n",
    "                self.load_para = True\n",
    "\n",
    "        else:\n",
    "            # 状态转移概率（状态->状态的条件概率）\n",
    "            self.A_dic = {}\n",
    "            # 发射概率（状态->词语的条件概率）\n",
    "            self.B_dic = {}\n",
    "            # 状态的初始概率\n",
    "            self.Pi_dic = {}\n",
    "            self.load_para = False\n",
    "\n",
    "    # 计算转移概率、发射概率以及初始概率\n",
    "    def train(self, path):\n",
    "\n",
    "        # 重置几个概率矩阵\n",
    "        self.try_load_model(False)\n",
    "\n",
    "        # 统计状态出现次数，求p(o)\n",
    "        Count_dic = {}\n",
    "\n",
    "        # 初始化参数\n",
    "        def init_parameters():\n",
    "            for state in self.state_list:\n",
    "                self.A_dic[state] = {s: 0.0 for s in self.state_list}\n",
    "                self.Pi_dic[state] = 0.0\n",
    "                self.B_dic[state] = {}\n",
    "\n",
    "                Count_dic[state] = 0\n",
    "\n",
    "        def makeLabel(text):\n",
    "            out_text = []\n",
    "            if len(text) == 1:\n",
    "                out_text.append('S')\n",
    "            else:\n",
    "                out_text += ['B'] + ['M'] * (len(text) - 2) + ['E']\n",
    "\n",
    "            return out_text\n",
    "\n",
    "        init_parameters()\n",
    "        line_num = -1\n",
    "        # 观察者集合，主要是字以及标点等\n",
    "        words = set()\n",
    "        with open(path, encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                line_num += 1\n",
    "\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                word_list = [i for i in line if i != ' ']\n",
    "                words |= set(word_list)  # 更新字的集合\n",
    "\n",
    "                linelist = line.split()\n",
    "\n",
    "                line_state = []\n",
    "                for w in linelist:\n",
    "                    line_state.extend(makeLabel(w))\n",
    "                \n",
    "                assert len(word_list) == len(line_state) #用来让程序测试这个condition，如果condition为false，那么raise一个AssertionError出来\n",
    "\n",
    "                for k, v in enumerate(line_state): # 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "                    Count_dic[v] += 1\n",
    "                    if k == 0:\n",
    "                        self.Pi_dic[v] += 1  # 每个句子的第一个字的状态，用于计算初始状态概率\n",
    "                    else:\n",
    "                        self.A_dic[line_state[k - 1]][v] += 1  # 计算转移概率\n",
    "                        self.B_dic[line_state[k]][word_list[k]] = \\\n",
    "                            self.B_dic[line_state[k]].get(word_list[k], 0) + 1.0  # 计算发射概率\n",
    "        \n",
    "        self.Pi_dic = {k: v * 1.0 / line_num for k, v in self.Pi_dic.items()}\n",
    "        self.A_dic = {k: {k1: v1 / Count_dic[k] for k1, v1 in v.items()}\n",
    "                      for k, v in self.A_dic.items()}\n",
    "        #加1平滑\n",
    "        self.B_dic = {k: {k1: (v1 + 1) / Count_dic[k] for k1, v1 in v.items()}\n",
    "                      for k, v in self.B_dic.items()}\n",
    "        #序列化\n",
    "        import pickle\n",
    "        with open(self.model_file, 'wb') as f:\n",
    "            pickle.dump(self.A_dic, f)\n",
    "            pickle.dump(self.B_dic, f)\n",
    "            pickle.dump(self.Pi_dic, f)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def viterbi(self, text, states, start_p, trans_p, emit_p):\n",
    "        V = [{}]\n",
    "        path = {}\n",
    "        for y in states:\n",
    "            V[0][y] = start_p[y] * emit_p[y].get(text[0], 0)\n",
    "            path[y] = [y]\n",
    "        for t in range(1, len(text)):\n",
    "            V.append({})\n",
    "            newpath = {}\n",
    "            \n",
    "            #检验训练的发射概率矩阵中是否有该字\n",
    "            neverSeen = text[t] not in emit_p['S'].keys() and \\\n",
    "                text[t] not in emit_p['M'].keys() and \\\n",
    "                text[t] not in emit_p['E'].keys() and \\\n",
    "                text[t] not in emit_p['B'].keys()\n",
    "            for y in states:\n",
    "                emitP = emit_p[y].get(text[t], 0) if not neverSeen else 1.0 #设置未知字单独成词\n",
    "                (prob, state) = max(\n",
    "                    [(V[t - 1][y0] * trans_p[y0].get(y, 0) *\n",
    "                      emitP, y0)\n",
    "                     for y0 in states if V[t - 1][y0] > 0])\n",
    "                V[t][y] = prob\n",
    "                newpath[y] = path[state] + [y]\n",
    "            path = newpath\n",
    "            \n",
    "        if emit_p['M'].get(text[-1], 0)> emit_p['S'].get(text[-1], 0):\n",
    "            (prob, state) = max([(V[len(text) - 1][y], y) for y in ('E','M')])\n",
    "        else:\n",
    "            (prob, state) = max([(V[len(text) - 1][y], y) for y in states])\n",
    "        \n",
    "        return (prob, path[state])\n",
    "\n",
    "    def cut(self, text):\n",
    "        import os\n",
    "        if not self.load_para:\n",
    "            self.try_load_model(os.path.exists(self.model_file))\n",
    "        prob, pos_list = self.viterbi(text, self.state_list, self.Pi_dic, self.A_dic, self.B_dic)      \n",
    "        begin, next = 0, 0    \n",
    "        for i, char in enumerate(text): # 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标\n",
    "            pos = pos_list[i]\n",
    "            if pos == 'B':\n",
    "                begin = i\n",
    "            elif pos == 'E':\n",
    "                yield text[begin: i+1]\n",
    "                next = i+1\n",
    "            elif pos == 'S':\n",
    "                yield char\n",
    "                next = i+1\n",
    "        if next < len(text):\n",
    "            yield text[next:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常棒的方案！\n",
      "['这是', '一个', '非常', '棒', '的', '方案', '！']\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM()\n",
    "\n",
    "hmm.train('learning-nlp/chapter-3/data/trainCorpus.txt_utf8')\n",
    "\n",
    "text = '这是一个非常棒的方案！'\n",
    "res = hmm.cut(text)\n",
    "print(text)\n",
    "print(str(list(res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 其他统计分词算法\n",
    "\n",
    "条件随机场（CRF），也是一种基于马尔可夫思想的统计模型。该算法使得每个状态不止与他前面的状态有关，还与他后面的状态有关。\n",
    "\n",
    "## 3.4 混合分词\n",
    "\n",
    "事实上，目前不管是基于规则的算法、还是基于HMM、CRF或者deep learning等的方法，其分词效果在具体任务中，差距并没有那么明显。在实际应用中，最常用的方式就是先基于词典的方式进行分词，然后再用统计分词方法进行辅助。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 中文分词工具——jieba\n",
    "jieba分词官网地址是*https://github.com/fxsjy/jieba*\n",
    "\n",
    "### 3.5.1 jieba的三种分词模式\n",
    "\n",
    "+ 精确模式：试图将句子最精确地切开，适合文本分析\n",
    "\n",
    "+ 全模式：把句子中所以可以成词的词语都扫描出来，速度非常快，但是不嫩解决歧义。\n",
    "\n",
    "+ 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。\n",
    "\n",
    "可使用 jieba.cut 和 jieba.cut_for_search 方法进行分词，两者所返回的结构都是一个可迭代的 generator，或者直接使用 jieba.lcut 以及 jieba.lcut_for_search 直接返回 list。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\62318\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.339 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "全模式： 中文/分词/是/文本/文本处理/本处/处理/不可/缺少/的/一步/！\n",
      "全模式(返回列表): ['中文', '分词', '是', '文本', '文本处理', '本处', '处理', '不可', '缺少', '的', '一步', '！']\n",
      "精确模式： 中文/分词/是/文本处理/不可/缺少/的/一步/！\n",
      "默认精确模式： 中文/分词/是/文本处理/不可/缺少/的/一步/！\n",
      "搜索引擎模式： 中文/分词/是/文本/本处/处理/文本处理/不可/缺少/的/一步/！\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "sent = '中文分词是文本处理不可缺少的一步！'\n",
    "seg_list = jieba.cut(sent, cut_all = True)\n",
    "print('全模式：', '/'.join(seg_list))\n",
    "type(seg_list)\n",
    "\n",
    "seg_llist = jieba.lcut(sent, cut_all = True)\n",
    "print('全模式(返回列表): {0}'.format(seg_llist))\n",
    "\n",
    "\n",
    "seg_list = jieba.cut(sent, cut_all= False)\n",
    "print('精确模式：', '/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut(sent)\n",
    "print('默认精确模式：', '/'.join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(sent)\n",
    "print('搜索引擎模式：', '/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 jieba 中，对于未登录到词库的词，使用了基于汉字成词能力的 HMM 模型和 Viterbi 算法，其大致原理是：采用四个隐含状态，分别表示为单字成词，词组的开头，词组的中间，词组的结尾。通过标注好的分词训练集，可以得到 HMM 的各个参数，然后使用 Viterbi 算法来解释测试集，得到分词结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 添加自定义词典\n",
    "\n",
    "开发者可以指定自定义词典，以便包含 jieba 词库里没有的词，词典格式如下：\n",
    "\n",
    "词语 词频（可省略） 词性（可省略）\n",
    "\n",
    "例如：\n",
    "\n",
    "创新办 3 i\n",
    "\n",
    "云计算 5\n",
    "\n",
    "凱特琳 nz\n",
    "\n",
    "虽然 jieba 有新词识别能力，但自行添加新词可以保证更高的正确率\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入词典\n",
    "\n",
    "使用 jieba.load_userdict(file_name) 即可载入词典。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【未加载词典】：周大福/ 是/ 创新/ 办/ 主任/ 也/ 是/ 云/ 计算/ 方面/ 的/ 专家\n",
      "【加载词典后】：周大福/ 是/ 创新办/ 主任/ 也/ 是/ 云计算/ 方面/ 的/ 专家\n"
     ]
    }
   ],
   "source": [
    "#示例 文本\n",
    "sample_text = \"周大福是创新办主任也是云计算方面的专家\"\n",
    "# 未加载词典\n",
    "print(\"【未加载词典】：\" + '/ '.join(jieba.cut(sample_text)))\n",
    "\n",
    "# 载入词典\n",
    "jieba.load_userdict(\"./userdict.txt\")\n",
    "\n",
    "# 加载词典后\n",
    "print(\"【加载词典后】：\" + '/ '.join(jieba.cut(sample_text)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调整词典\n",
    "\n",
    "使用 add_word(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【调节词频前】：如果/放到/post/中将/出错/。\n",
      "【调节词频后】：如果/放到/post/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('石墨烯') #增加自定义词语\n",
    "jieba.add_word('凱特琳', freq=42, tag='nz') #设置词频和词性 \n",
    "jieba.del_word('自定义词') #删除自定义词语 \n",
    "# 调节词频前\n",
    "print(\"【调节词频前】：\" + '/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))\n",
    "\n",
    "# 调节词频\n",
    "jieba.suggest_freq(('中', '将'), True)\n",
    "# 调节词频后\n",
    "print(\"【调节词频后】：\" + '/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 关键词提取\n",
    "jieba 提供了两种关键词提取方法，分别基于 TF-IDF 算法和 TextRank 算法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TF-IDF 算法的关键词提取\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率)是一种统计方法，用以评估一个词语对于一个文件集或一个语料库中的一份文件的重要程度，其原理可概括为：\n",
    "\n",
    "一个词语在一篇文章中出现次数越多，同时在所有文档中出现次数越少，越能够代表该文章。\n",
    "\n",
    "计算公式：TF-IDF = TF * IDF，其中：\n",
    "\n",
    "+ TF(term frequency, TF)：词频，某一个给定的词语在该文件中出现的次数，计算公式：$$ TF_\\omega = \\frac{在某类中词条\\omega 出现的次数}{该类所有的词条数目}$$或$$ TF = \\frac{某个词在文章中出现次数}{文章的总词数}$$\n",
    "\n",
    "+ IDF(inverse document frequency, IDF)：逆文件频率，如果包含词条的文件越少，则说明词条具有很好的类别区分能力，计算公式：$$ IDF = log(\\frac{语料库的文档总数}{包含词条\\omega的文档数+1}) $$\n",
    "\n",
    "如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 jieba.analyse.extract_tags 方法可以基于 TF-IDF 算法进行关键词提取，该方法共有 4 个参数：\n",
    "\n",
    "+ sentence：为待提取的文本\n",
    "\n",
    "+ topK：根据tf-idf值对词频词典中的词进行降序排序，然后输出topK个词作为关键词，默认值为 20\n",
    "\n",
    "+ withWeight：是否一并返回关键词权重值，默认值为 False\n",
    "\n",
    "+ allowPOS：仅包括指定词性的词，默认值为空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 0.7300142700289363\n",
      "吉林 0.659038184373617\n",
      "置业 0.4887134522112766\n",
      "万元 0.3392722481859574\n",
      "增资 0.33582401985234045\n",
      "4.3 0.25435675538085106\n",
      "7000 0.25435675538085106\n",
      "2013 0.25435675538085106\n",
      "139.13 0.25435675538085106\n",
      "实现 0.19900979900382978\n",
      "综合体 0.19480309624702127\n",
      "经营范围 0.19389757253595744\n",
      "亿元 0.1914421623587234\n",
      "在建 0.17541884768425534\n",
      "全资 0.17180164988510638\n",
      "注册资本 0.1712441526\n",
      "百货 0.16734460041382979\n",
      "零售 0.1475057117057447\n",
      "子公司 0.14596045237787234\n",
      "营业 0.13920178509021275\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "import jieba.analyse as anls\n",
    "s = \"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\"\n",
    "\n",
    "for x, w in anls.extract_tags(s, topK=20, withWeight=True):\n",
    "    print('%s %s' % (x, w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TextRank 算法的关键词提取\n",
    "\n",
    "TextRank 是另一种关键词提取算法，基于大名鼎鼎的 PageRank，以词为节点，以共现关系建立起节点之间的链接。TextRank实际上是依据位置与词频来计算词的权重的。\n",
    "\n",
    "通过 jieba.analyse.textrank 方法可以使用基于 TextRank 算法的关键词提取，其与 'jieba.analyse.extract_tags' 有一样的参数，但前者默认过滤词性（allowPOS=('ns', 'n', 'vn', 'v')）。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吉林 1.0\n",
      "欧亚 0.9966893354178172\n",
      "置业 0.6434360313092776\n",
      "实现 0.5898606692859626\n",
      "收入 0.43677859947991454\n",
      "增资 0.4099900531283276\n",
      "子公司 0.35678295947672795\n",
      "城市 0.34971383667403655\n",
      "商业 0.34817220716026936\n",
      "业务 0.3092230992619838\n",
      "在建 0.3077929164033088\n",
      "营业 0.3035777049319588\n",
      "全资 0.303540981053475\n",
      "综合体 0.29580869172394825\n",
      "注册资本 0.29000519464085045\n",
      "有限公司 0.2807830798576574\n",
      "零售 0.27883620861218145\n",
      "百货 0.2781657628445476\n",
      "开发 0.2693488779295851\n",
      "经营范围 0.2642762173558316\n"
     ]
    }
   ],
   "source": [
    "for x, w in anls.textrank(s, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 自定义语料库\n",
    "关键词提取所使用逆向文件频率（IDF）文本语料库和停止词（Stop Words）文本语料库可以切换成自定义语料库的路径。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吉林 1.0174270215234043\n",
      "欧亚 1.0174270215234043\n",
      "置业 0.7630702661425532\n",
      "万元 0.7630702661425532\n",
      "增资 0.5087135107617021\n",
      "亿元 0.5087135107617021\n",
      "实现 0.5087135107617021\n",
      "此外 0.25435675538085106\n",
      "公司 0.25435675538085106\n",
      "全资 0.25435675538085106\n",
      "子公司 0.25435675538085106\n",
      "有限公司 0.25435675538085106\n",
      "4.3 0.25435675538085106\n",
      "注册资本 0.25435675538085106\n",
      "7000 0.25435675538085106\n",
      "增加 0.25435675538085106\n",
      "主要 0.25435675538085106\n",
      "经营范围 0.25435675538085106\n",
      "房地产 0.25435675538085106\n",
      "开发 0.25435675538085106\n"
     ]
    }
   ],
   "source": [
    "jieba.analyse.set_stop_words(\"userdict.txt\")\n",
    "jieba.analyse.set_idf_path(\"idf.txt.big\")\n",
    "for x, w in anls.extract_tags(s, topK=20, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 词性标注\n",
    "\n",
    "jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器。\n",
    "\n",
    "+ tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他 r\n",
      "改变 v\n",
      "了 ul\n",
      "中国 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"他改变了中国\")\n",
    "for word, flag in words:\n",
    "    print(\"{0} {1}\".format(word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词性对照表（allowPOS可选值）\n",
    "##### 名词 (1个一类，7个二类，5个三类)\n",
    "名词分为以下子类：\n",
    "n 名词\n",
    "nr 人名\n",
    "nr1 汉语姓氏\n",
    "nr2 汉语名字\n",
    "nrj 日语人名\n",
    "nrf 音译人名\n",
    "ns 地名\n",
    "nsf 音译地名\n",
    "nt 机构团体名\n",
    "nz 其它专名\n",
    "nl 名词性惯用语\n",
    "ng 名词性语素\n",
    "\n",
    "##### 时间词(1个一类，1个二类)\n",
    "t 时间词\n",
    "tg 时间词性语素\n",
    "\n",
    "##### 处所词(1个一类)\n",
    "s 处所词\n",
    "\n",
    "##### 方位词(1个一类)\n",
    "f 方位词\n",
    "\n",
    "##### 动词(1个一类，9个二类)\n",
    "v 动词\n",
    "vd 副动词\n",
    "vn 名动词\n",
    "vshi 动词“是”\n",
    "vyou 动词“有”\n",
    "vf 趋向动词\n",
    "vx 形式动词\n",
    "vi 不及物动词（内动词）\n",
    "vl 动词性惯用语\n",
    "vg 动词性语素\n",
    "\n",
    "##### 形容词(1个一类，4个二类)\n",
    "a 形容词\n",
    "ad 副形词\n",
    "an 名形词\n",
    "ag 形容词性语素\n",
    "al 形容词性惯用语\n",
    "\n",
    "#####  区别词(1个一类，2个二类)\n",
    "b 区别词\n",
    "bl 区别词性惯用语\n",
    "\n",
    "##### 状态词(1个一类)\n",
    "z 状态词\n",
    "\n",
    "##### 代词(1个一类，4个二类，6个三类)\n",
    "r 代词\n",
    "rr 人称代词\n",
    "rz 指示代词\n",
    "rzt 时间指示代词\n",
    "rzs 处所指示代词\n",
    "rzv 谓词性指示代词\n",
    "ry 疑问代词\n",
    "ryt 时间疑问代词\n",
    "rys 处所疑问代词\n",
    "ryv 谓词性疑问代词\n",
    "rg 代词性语素\n",
    "\n",
    "##### 数词(1个一类，1个二类)\n",
    "m 数词\n",
    "mq 数量词\n",
    "\n",
    "##### 量词(1个一类，2个二类)\n",
    "q 量词\n",
    "qv 动量词\n",
    "qt 时量词\n",
    "\n",
    "##### 副词(1个一类)\n",
    "d 副词\n",
    "\n",
    "##### 介词(1个一类，2个二类)\n",
    "p 介词\n",
    "pba 介词“把”\n",
    "pbei 介词“被”\n",
    "\n",
    "##### 连词(1个一类，1个二类)\n",
    "c 连词\n",
    "cc 并列连词\n",
    "\n",
    "##### 助词(1个一类，15个二类)\n",
    "u 助词\n",
    "uzhe 着\n",
    "ule 了 喽\n",
    "uguo 过\n",
    "ude1 的 底\n",
    "ude2 地\n",
    "ude3 得\n",
    "usuo 所\n",
    "udeng 等 等等 云云\n",
    "uyy 一样 一般 似的 般\n",
    "udh 的话\n",
    "uls 来讲 来说 而言 说来\n",
    "uzhi 之\n",
    "ulian 连 （“连小学生都会”）\n",
    "\n",
    "##### 叹词(1个一类)\n",
    "e 叹词\n",
    "\n",
    "##### 语气词(1个一类)\n",
    "y 语气词(delete yg)\n",
    "\n",
    "##### 拟声词(1个一类)\n",
    "o 拟声词\n",
    "前缀(1个一类)\n",
    "\n",
    "h 前缀\n",
    "##### 后缀(1个一类)\n",
    "k 后缀\n",
    "\n",
    "##### 字符串(1个一类，2个二类)\n",
    "x 字符串\n",
    "xx 非语素字\n",
    "xu 网址URL\n",
    "\n",
    "##### 标点符号(1个一类，16个二类)\n",
    "w 标点符号\n",
    "wkz 左括号，全角：（ 〔 ［ ｛ 《 【 〖 〈 半角：( \\[ { <\n",
    "wky 右括号，全角：） 〕 ］ ｝ 》 】 〗 〉 半角： ) \\] { >\n",
    "wyz 左引号，全角：“ ‘ 『\n",
    "wyy 右引号，全角：” ’ 』\n",
    "wj 句号，全角：。\n",
    "ww 问号，全角：？ 半角：?\n",
    "wt 叹号，全角：！ 半角：!\n",
    "wd 逗号，全角：， 半角：,\n",
    "wf 分号，全角：； 半角： ;\n",
    "wn 顿号，全角：、\n",
    "wm 冒号，全角：： 半角： :\n",
    "ws 省略号，全角：…… …\n",
    "wp 破折号，全角：—— －－ ——－ 半角：— —-\n",
    "wb 百分号千分号，全角：％ ‰ 半角：%\n",
    "wh 单位符号，全角：￥ ＄ ￡ ° ℃ 半角：$\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
